{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import opensfm\n",
    "import cv2\n",
    "from opensfm import dataset, features, io\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import ngransac\n",
    "\n",
    "from network import CNNet\n",
    "from dataset import SparseDataset\n",
    "import util\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/projects/OpenSfM/data/gezer/'\n",
    "data = dataset.DataSet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v2 dji fc300s 4000 3000 perspective 0.5555': PerspectiveCamera('v2 dji fc300s 4000 3000 perspective 0.5555', 'perspective', 4000, 3000, 0.5555555555555556, 0.0, 0.0, 0.5555555555555556, 0.0, 0.0)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = data.load_camera_models(); camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_name = list(camera.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerspectiveCamera('v2 dji fc300s 4000 3000 perspective 0.5555', 'perspective', 4000, 3000, 0.5555555555555556, 0.0, 0.0, 0.5555555555555556, 0.0, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = camera[camera_name]; camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.22222222e+03, 0.00000000e+00, 1.99950000e+03],\n",
       "       [0.00000000e+00, 2.22222222e+03, 1.49950000e+03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = camera.get_K_in_pixel_coordinates(); K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes: normalize_pts as defined in /home/ubuntu/projects/OpenSfM/opensfm/features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack stuff to directory ngransac:\n",
    "image1 = '../../ngransac/images/demo1.jpg'\n",
    "image2 = '../../ngransac/images/demo2.jpg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images\n",
    "img1 = cv2.imread(image1)\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "img2 = cv2.imread(image2)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[900. ,   0. , 640. ],\n",
       "       [  0. , 900. , 359.5],\n",
       "       [  0. ,   0. ,   1. ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focallength = 900.0\n",
    "K = np.eye(3)\n",
    "K[0,0] = K[1,1] = focallength\n",
    "K[0,2] = img1.shape[1] * 0.5\n",
    "K[1,2] = img1.shape[0] * 0.5\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of valid matches: 2000\n"
     ]
    }
   ],
   "source": [
    "nfeatures = 2000\n",
    "detector  = cv2.xfeatures2d.SIFT_create(nfeatures=nfeatures, contrastThreshold=1e-5)\n",
    "\n",
    "# detect features\n",
    "kp1, desc1 = detector.detectAndCompute(img1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(img2, None)\n",
    "\n",
    "# feature matching\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "\n",
    "good_matches = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "ratio=1.0 \n",
    "#side information for the network (matching ratios in this case)\n",
    "ratios = []\n",
    "\n",
    "print(\"\")\n",
    "if ratio < 1.0:\n",
    "\tprint(\"Using Lowe's ratio filter with\", ratio)\n",
    "\n",
    "for (m,n) in matches:\n",
    "\tif m.distance < ratio*n.distance: # apply Lowe's ratio filter\n",
    "\t\tgood_matches.append(m)\n",
    "\t\tpts2.append(kp2[m.trainIdx].pt)\n",
    "\t\tpts1.append(kp1[m.queryIdx].pt)\n",
    "\t\tratios.append(m.distance / n.distance)\n",
    "\n",
    "print(\"Number of valid matches:\", len(good_matches))\n",
    "\n",
    "pts1 = np.array([pts1])\n",
    "pts2 = np.array([pts2])\n",
    "\n",
    "ratios = np.array([ratios])\n",
    "ratios = np.expand_dims(ratios, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGRansac:\n",
    "    \n",
    "    # init model with default params. Calculate only fundamental matrix\n",
    "    def __init__(self,\n",
    "                 frame_size,           \n",
    "                 K = None,\n",
    "                 batchsize=32,\n",
    "                 fmat=False,\n",
    "                 hyps=1000,\n",
    "                 model='',\n",
    "                 nfeatures=2000,\n",
    "                 nosideinfo=False,                 \n",
    "                 orb=False,\n",
    "                 ratio=1.0,\n",
    "                 refine=False,\n",
    "                 resblocks=12,\n",
    "                 rootsift=False,\n",
    "                 session='',\n",
    "                 threshold=0.001):\n",
    "        \n",
    "        print(K)\n",
    "        if fmat:\n",
    "            print(\"\\nFitting Fundamental Matrix...\\n\")\n",
    "        else:\n",
    "            print(\"\\nFitting Essential Matrix...\\n\")\n",
    "        \n",
    "        # load network\n",
    "        model_file = model\n",
    "        if len(model_file) == 0:\n",
    "            model_file = util.create_session_string('e2e', fmat, orb, rootsift, ratio, session)\n",
    "            model_file = '../../ngransac/models/weights_' + model_file + '.net'            \n",
    "            print('Loading pre-trained model:', model_file)\n",
    "            \n",
    "        model = CNNet(resblocks)\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        model = model.cuda()\n",
    "        model.eval()        \n",
    "        print('model ', model_file, ' successfully loaded.')                                    \n",
    "        \n",
    "        self.model      = model\n",
    "        self.frame_size = frame_size\n",
    "        self.nosideinfo = nosideinfo\n",
    "        self.hyps       = hyps\n",
    "        self.threshold  = threshold\n",
    "        self.refine     = refine\n",
    "        self.K = K\n",
    "        \n",
    "    \n",
    "    def findEssentialMat(self, pts1, pts2, ratios):\n",
    "        \n",
    "        print(self.K)\n",
    "        assert self.K is not None, 'To calculate E, K must be NOT NONE'\n",
    "        \n",
    "        pts1 = cv2.undistortPoints(pts1, self.K, None)\n",
    "        pts2 = cv2.undistortPoints(pts2, self.K, None)\n",
    "        \n",
    "        \n",
    "        if self.nosideinfo:\n",
    "            # remove side information before passing it to the network\n",
    "            ratios = np.zeros(ratios.shape)\n",
    "        \n",
    "        # create data tensor of feature coordinates and matching ratios\n",
    "        correspondences = np.concatenate((pts1, pts2, ratios), axis=2)\n",
    "        correspondences = np.transpose(correspondences)\n",
    "        correspondences = torch.from_numpy(correspondences).float()\n",
    "\n",
    "        # predict neural guidance, i.e. RANSAC sampling probabilities\n",
    "        log_probs = self.model(correspondences.unsqueeze(0).cuda())[0] #zero-indexing creates and removes a dummy batch dimension\n",
    "        probs = torch.exp(log_probs).cpu()\n",
    "\n",
    "        out_model     = torch.zeros((3, 3)).float() # estimated model\n",
    "        out_inliers   = torch.zeros(log_probs.size()) # inlier mask of estimated model\n",
    "        out_gradients = torch.zeros(log_probs.size()) # gradient tensor (only used during training)\n",
    "        rand_seed     = 0 # random seed to by used in C++\n",
    "        \n",
    "        \n",
    "        incount = ngransac.find_essential_mat(correspondences, \n",
    "                                              probs, \n",
    "                                              rand_seed, \n",
    "                                              self.hyps, \n",
    "                                              self.threshold, \n",
    "                                              out_model, \n",
    "                                              out_inliers, \n",
    "                                              out_gradients)\n",
    "        \n",
    "        print(\"\\n=== Model found by NG-RANSAC: =======\\n\")\n",
    "        print(\"\\nNG-RANSAC Inliers: \", int(incount))\n",
    "        \n",
    "        # Fundamental matrix\n",
    "        return out_model.numpy()\n",
    "    \n",
    "    # pts1 and pts2 must be normalized to the frame size before running the procedure below\n",
    "    # as well the ratios must be computed\n",
    "    def findFundamentalMat(self, pts1, pts2, ratios):\n",
    "        \n",
    "        # normalize x and y coordinates before passing them to the network\n",
    "        # normalized by the image size\n",
    "        util.normalize_pts(pts1, self.frame_size)\n",
    "        util.normalize_pts(pts2, self.frame_size)\n",
    "        \n",
    "        if self.nosideinfo:\n",
    "            # remove side information before passing it to the network\n",
    "            ratios = np.zeros(ratios.shape)\n",
    "        \n",
    "        # create data tensor of feature coordinates and matching ratios\n",
    "        correspondences = np.concatenate((pts1, pts2, ratios), axis=2)\n",
    "        correspondences = np.transpose(correspondences)\n",
    "        correspondences = torch.from_numpy(correspondences).float()\n",
    "\n",
    "        # predict neural guidance, i.e. RANSAC sampling probabilities\n",
    "        log_probs = self.model(correspondences.unsqueeze(0).cuda())[0] #zero-indexing creates and removes a dummy batch dimension\n",
    "        probs = torch.exp(log_probs).cpu()\n",
    "\n",
    "        out_model     = torch.zeros((3, 3)).float() # estimated model\n",
    "        out_inliers   = torch.zeros(log_probs.size()) # inlier mask of estimated model\n",
    "        out_gradients = torch.zeros(log_probs.size()) # gradient tensor (only used during training)\n",
    "        rand_seed     = 0 # random seed to by used in C++\n",
    "        \n",
    "        # run NG-RANSAC\n",
    "        # === CASE FUNDAMENTAL MATRIX =========================================\n",
    "\n",
    "        # undo normalization of x and y image coordinates\n",
    "        util.denormalize_pts(correspondences[0:2], self.frame_size)\n",
    "        util.denormalize_pts(correspondences[2:4], self.frame_size)\n",
    "        \n",
    "        incount = ngransac.find_fundamental_mat(correspondences, \n",
    "                                                probs, \n",
    "                                                rand_seed, \n",
    "                                                self.hyps, \n",
    "                                                self.threshold, \n",
    "                                                self.refine, \n",
    "                                                out_model, \n",
    "                                                out_inliers, \n",
    "                                                out_gradients)\n",
    "        \n",
    "        print(\"\\n=== Model found by NG-RANSAC: =======\\n\")\n",
    "        print(\"\\nNG-RANSAC Inliers: \", int(incount))\n",
    "        \n",
    "        # Fundamental matrix\n",
    "        return out_model.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[900.    0.  640. ]\n",
      " [  0.  900.  359.5]\n",
      " [  0.    0.    1. ]]\n",
      "\n",
      "Fitting Essential Matrix...\n",
      "\n",
      "Loading pre-trained model: ../../ngransac/models/weights_e2e_E_r1.00_.net\n",
      "model  ../../ngransac/models/weights_e2e_E_r1.00_.net  successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "ng = NGRansac(frame_size=img1.shape, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[900.    0.  640. ]\n",
      " [  0.  900.  359.5]\n",
      " [  0.    0.    1. ]]\n",
      "\n",
      "=== Model found by NG-RANSAC: =======\n",
      "\n",
      "\n",
      "NG-RANSAC Inliers:  121\n"
     ]
    }
   ],
   "source": [
    "E = ng.findEssentialMat(pts1, pts2, ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E =  [[ 0.01791449  0.6465942  -0.0030416 ]\n",
      " [-0.7030774   0.009333   -0.07142729]\n",
      " [ 0.01623759 -0.28600842  0.00379763]]\n"
     ]
    }
   ],
   "source": [
    "print('E = ', E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
